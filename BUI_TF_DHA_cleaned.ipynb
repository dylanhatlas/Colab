{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BUI_TF_DHA_cleaned",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanhatlas/Colab/blob/master/BUI_TF_DHA_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "EWNrXFBQ5Eaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from __future__ import print_function\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P8V5Utv-QseZ",
        "colab_type": "code",
        "outputId": "486696d8-733d-4a55-baab-b39b680a2e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.VERSION"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "_ivLm6urQfjd",
        "colab_type": "code",
        "outputId": "49c784b1-4ffe-434d-efcd-a99f59469c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import tensorflow_transform as tft\n",
        "except ImportError:\n",
        "  # this will take a minute, ignore the warnings\n",
        "  !pip install -q tensorflow-transform\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam.impl as tft_beam\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import dataset_schema"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 174kB 6.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 2.5MB 8.8MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 17.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 133kB 26.8MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 18.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 11.6MB 2.4MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 16.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 81kB 24.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 337kB 21.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 92kB 26.4MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 163kB 28.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 102kB 25.0MB/s \n",
            "\u001b[?25h  Building wheel for tensorflow-transform (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for pydot (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for avro (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for oauth2client (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for pyvcf (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for proto-google-cloud-datastore-v1 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for googledatastore (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[31mgoogle-cloud-storage 1.13.2 has requirement google-cloud-core<0.30dev,>=0.29.0, but you'll have google-cloud-core 0.28.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-cloud-translate 1.3.3 has requirement google-cloud-core<0.30dev,>=0.29.0, but you'll have google-cloud-core 0.28.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8S8BTpqGGNKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "d0c6021a-c738-4193-8f1f-f7abf700a869"
      },
      "cell_type": "code",
      "source": [
        "!pip install earthengine-api\n",
        "!earthengine authenticate --quiet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting earthengine-api\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/85/2c38a4fec2871abe9a7500eeaca946e906d0f7dc0304e0de2691550584fa/earthengine-api-0.1.170.tar.gz (115kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python2.7/dist-packages (from earthengine-api) (1.6.7)\n",
            "Collecting pyOpenSSL>=0.11 (from earthengine-api)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from earthengine-api) (1.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client->earthengine-api) (0.11.3)\n",
            "Requirement already satisfied: oauth2client<5.0.0dev,>=1.5.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client->earthengine-api) (4.1.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client->earthengine-api) (3.0.0)\n",
            "Collecting cryptography>=2.3 (from pyOpenSSL>=0.11->earthengine-api)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/c1/cf8665c955c9393e9ff0872ba6cd3dc6f46ef915e94afcf6e0410508ca69/cryptography-2.6.1-cp27-cp27mu-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client->earthengine-api) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client->earthengine-api) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client->earthengine-api) (0.2.4)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.3->pyOpenSSL>=0.11->earthengine-api)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 27.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: enum34; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from cryptography>=2.3->pyOpenSSL>=0.11->earthengine-api) (1.1.6)\n",
            "Requirement already satisfied: ipaddress; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from cryptography>=2.3->pyOpenSSL>=0.11->earthengine-api) (1.0.22)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python2.7/dist-packages (from cryptography>=2.3->pyOpenSSL>=0.11->earthengine-api) (1.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python2.7/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3->pyOpenSSL>=0.11->earthengine-api) (2.19)\n",
            "Building wheels for collected packages: earthengine-api\n",
            "  Building wheel for earthengine-api (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/30/46/64/6ef61daf0cee07ac0dc0d9c041d4cce9ce73052191bf8d6bc2\n",
            "Successfully built earthengine-api\n",
            "Installing collected packages: asn1crypto, cryptography, pyOpenSSL, earthengine-api\n",
            "Successfully installed asn1crypto-0.24.0 cryptography-2.6.1 earthengine-api-0.1.170 pyOpenSSL-19.0.0\n",
            "Paste the following address into a web browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com\n",
            "\n",
            "On the web page, please authorize access to your Earth Engine account and copy the authentication code. Next authenticate with the following command:\n",
            "\n",
            "    earthengine authenticate --authorization-code=PLACE_AUTH_CODE_HERE\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G5fMmRxvF9K0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a7f39cd-a65a-4d89-d048-6c7039727ea6"
      },
      "cell_type": "code",
      "source": [
        "!earthengine authenticate --authorization-code=4/BgHNiAgB4um62F5Gt7rBB7XZpjhW1EDzQ9EBkYfYfhVK8cuHPG2ZoCg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully saved authorization token.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bLgAOLWc5K-s",
        "colab_type": "code",
        "outputId": "e12f020f-2f9f-4e6f-bf0f-53744f32c0f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "hello = tf.constant('Hello world!')\n",
        "with tf.Session() as sess:\n",
        "  print(sess.run(hello))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F851NkAi5XLP",
        "colab_type": "code",
        "outputId": "8d911f90-2a78-40af-9040-7348b38f3f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "!./ngrok authtoken 37RLp3AHZcgpGBdpoESNs_5rTLmZD8Xfm3Zr2fSLNd5\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-06 17:17:01--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.203.53.176, 52.200.123.104, 52.203.102.189, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.203.53.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14809752 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.12M  6.98MB/s    in 2.0s    \n",
            "\n",
            "2019-03-06 17:17:03 (6.98 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14809752/14809752]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BNL3P9H75gOr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_dir = '/tmp/BUI2'\n",
        "\n",
        "LOG_DIR = model_dir\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n1RDT2kD5q2K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Configure tunnel to tensorboard\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wyGavFHb5tY8",
        "colab_type": "code",
        "outputId": "839804f3-37d4-44e0-90af-1a4a5e4c11e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python2 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://49392b91.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EfRbMG8d5t30",
        "colab_type": "code",
        "outputId": "9018134f-8033-44de-9a16-6a6d099ff94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "! npm install -g localtunnel"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/client\n",
            "+ localtunnel@1.9.1\n",
            "added 55 packages from 34 contributors in 2.396s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HA30oJg-5t6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ! top"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4F3I28y5t9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Kill Tensorboard port if desired\n",
        "# ! ps -ef | grep 6006 #find port\n",
        "# ! kill -9 6349 #specify and close port"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDaw6sxZ8X_t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# First, we need to set our project. Replace the assignment below\n",
        "# with your project ID.\n",
        "project_id = 'unearthing-deep-learning'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zhZyBFi6889S",
        "colab_type": "code",
        "outputId": "2ccd8b0f-a73b-4eec-8359-aa1a113b4f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iArirT3X8YS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bucket_name = 'geo_ai'\n",
        "link =  'aqxyJqhGzwGuViEC3UgNWTnVgDML1Ygw' # '9a26cef21ab34f6257d0a250882124fc'\n",
        "extension = 'ee_export.tfrecord.gz'\n",
        "training_name = 'BUI_train_' + link + extension\n",
        "testing_name = 'BUI_test_' + link + extension"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BmMrggqb6NKu",
        "colab_type": "code",
        "outputId": "8aed6b67-9ee0-4e20-d639-7d093ffe5333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Download the traing and testing files.\n",
        "!gsutil cp gs://{bucket_name}/{training_name} /tmp/{training_name}\n",
        "!gsutil cp gs://{bucket_name}/{testing_name} /tmp/{testing_name}\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://geo_ai/BUI_train_aqxyJqhGzwGuViEC3UgNWTnVgDML1Ygwee_export.tfrecord.gz...\n",
            "/ [1 files][934.3 KiB/934.3 KiB]                                                \n",
            "Operation completed over 1 objects/934.3 KiB.                                    \n",
            "Copying gs://geo_ai/BUI_test_aqxyJqhGzwGuViEC3UgNWTnVgDML1Ygwee_export.tfrecord.gz...\n",
            "/ [1 files][238.8 KiB/238.8 KiB]                                                \n",
            "Operation completed over 1 objects/238.8 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bWkv6UzfQIRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_files = 'BUI_image_' + link\n",
        "\n",
        "# Download the prediction images.\n",
        "predict_files = !gsutil ls gs://{bucket_name}/{image_files}*.tfrecord.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V03LUpWN6NNm",
        "colab_type": "code",
        "outputId": "47faec47-0a42-4d9a-defe-d66ea2cfd0da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Inspect the TFRecord dataset\n",
        "\n",
        "train_file = '/tmp/' + training_name\n",
        "test_file = '/tmp/' + testing_name\n",
        "\n",
        "\n",
        "driveDataset = tf.data.TFRecordDataset(train_file, compression_type='GZIP')\n",
        "iterator = driveDataset.make_one_shot_iterator()\n",
        "foo = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "    print(sess.run([foo]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\nw\\n\\x16\\n\\nnlcd_urban\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x80?\\n\\x13\\n\\x07pop_den\\x12\\x08\\x12\\x06\\n\\x04\\xce\\xfd\\xccB\\n\\x1c\\n\\x10nighttime_lights\\x12\\x08\\x12\\x06\\n\\x04\\xa7t\\xa1@\\n\\x19\\n\\raccessibility\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x004B\\n\\x0f\\n\\x03BUI\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00ED']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x0QyXt-H6NQ2",
        "colab_type": "code",
        "outputId": "d5cb1385-9b90-46cd-bff3-84b5430b1c0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Define the structure of the training/testing data\n",
        "\n",
        "# Names of the features.\n",
        "bands = ['nlcd_urban','pop_den','nighttime_lights','accessibility']\n",
        "label = 'BUI'\n",
        "featureNames = list(bands)\n",
        "featureNames.append(label)\n",
        "\n",
        "# Feature columns\n",
        "columns = [\n",
        "  tf.FixedLenFeature(shape=[1], dtype=tf.float32) for k in featureNames\n",
        "]\n",
        "\n",
        "# Dictionary with names as keys, features as values.\n",
        "featuresDict = dict(zip(featureNames, columns))\n",
        "print(featuresDict)\n",
        "\n",
        "# Create metadata to be used in preprocessing function for transforming data to scale 0-1\n",
        "# raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
        "#     dataset_schema.from_feature_spec(featuresDict))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'pop_den': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'BUI': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'nlcd_urban': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'accessibility': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'nighttime_lights': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S8JOUiX6GQrz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Make and test a parsing function\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  parsed_features = tf.parse_single_example(example_proto, featuresDict)\n",
        "  labels = parsed_features.pop(label)\n",
        "  return parsed_features, tf.cast(labels, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kMJXVMKNGQ3Q",
        "colab_type": "code",
        "outputId": "eedd4669-397a-4c7d-88ad-1e5767610ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Map the function over the dataset\n",
        "parsedDataset = driveDataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "\n",
        "iterator = parsedDataset.make_one_shot_iterator()\n",
        "foo = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "    print(sess.run([foo]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[({'pop_den': array([102.49571], dtype=float32), 'nlcd_urban': array([1.], dtype=float32), 'accessibility': array([45.], dtype=float32), 'nighttime_lights': array([5.04549], dtype=float32)}, array([788.], dtype=float32))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "opxHNvs5tg7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataframe_from_tfrecords(tf_records_file):\n",
        "  \n",
        "  driveDataset = tf.data.TFRecordDataset(tf_records_file, compression_type='GZIP')\n",
        "  parsedDataset = driveDataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  # Choose a value of `max_elems` that is at least as large as the dataset.\n",
        "  max_elems = 100000\n",
        "  parsedDataset = parsedDataset.batch(max_elems)\n",
        "\n",
        "  # Extracts the single element of a dataset as one or more `tf.Tensor` objects.\n",
        "  # No iterator needed in this case!\n",
        "  whole_dataset_tensors = tf.data.experimental.get_single_element(parsedDataset)\n",
        "\n",
        "  # Create a session and evaluate `whole_dataset_tensors` to get arrays contained within a dictionary.\n",
        "  with tf.Session() as sess:\n",
        "    whole_dataset_arrays = sess.run(whole_dataset_tensors)\n",
        "      \n",
        "  # Create dictionary for input variables and their values\n",
        "  raw_dict = {}\n",
        "\n",
        "  for key in whole_dataset_arrays[0].keys():\n",
        "\n",
        "    #Prepare data for DataFrame\n",
        "    raw_dict[key] = whole_dataset_arrays[0][key].flatten().T\n",
        "    raw_dict[label] = whole_dataset_arrays[1].flatten().T\n",
        "    \n",
        "    raw_df = pd.DataFrame.from_dict(raw_dict, orient='columns')\n",
        "    \n",
        "  return raw_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gKe9MPv8G19R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "37938ed8-87ed-42fa-a8ef-48f1b8208723"
      },
      "cell_type": "code",
      "source": [
        "# @title Create dataframe and summary statistics of raw input data\n",
        "raw_df = create_dataframe_from_tfrecords(train_file)\n",
        "print(raw_df.head())\n",
        "\n",
        "raw_describe = raw_df.describe()\n",
        "print(raw_describe)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       BUI  accessibility  nighttime_lights  nlcd_urban     pop_den\n",
            "0    788.0           45.0          5.045490         1.0  102.495712\n",
            "1   3660.0           33.0          5.639481         0.0  108.417595\n",
            "2   1052.0           34.0          7.498560        22.0   68.167572\n",
            "3  26488.0           33.0          7.498560        24.0   74.555862\n",
            "4   4206.0           30.0          6.429911         0.0   42.455627\n",
            "                BUI  accessibility  nighttime_lights    nlcd_urban  \\\n",
            "count  4.513000e+04   45130.000000      45130.000000  45130.000000   \n",
            "mean   5.528390e+05      35.638733        102.080101     39.041260   \n",
            "std    8.695911e+06      49.787010        160.531372     36.708942   \n",
            "min    0.000000e+00       0.000000          0.000000      0.000000   \n",
            "25%    7.200000e+02       0.000000          6.297746      2.000000   \n",
            "50%    8.528000e+03      18.000000         21.049723     23.000000   \n",
            "75%    1.287568e+05      52.000000        142.536194     81.000000   \n",
            "max    1.006953e+09     731.000000       1674.735840    100.000000   \n",
            "\n",
            "            pop_den  \n",
            "count  45130.000000  \n",
            "mean    1188.967529  \n",
            "std     3927.046875  \n",
            "min        0.000000  \n",
            "25%       14.360545  \n",
            "50%       90.729328  \n",
            "75%      882.958618  \n",
            "max    64828.113281  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3nQCaOcXtqdV",
        "colab_type": "code",
        "outputId": "275901f8-ae93-4f29-a5ce-38bc8eb99282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "test_df = create_dataframe_from_tfrecords(test_file)\n",
        "print(test_df.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        BUI  accessibility  nighttime_lights  nlcd_urban     pop_den\n",
            "0  104829.0           23.0         30.240294        25.0  529.604492\n",
            "1    3605.0           39.0          4.781260        36.0   25.985250\n",
            "2    7503.0           37.0          6.079037         0.0    9.283508\n",
            "3    4560.0           18.0         13.680815         0.0   98.213531\n",
            "4  543643.0           14.0         21.560720        89.0  285.676239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1EBVfvKoiTR0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_scale_factor = 1e5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i4OHDe77ufCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title scale features and target variable the brute force way using sklearn transform\n",
        "\n",
        "def scale_dataframe_features_labels(df, features_to_scale, label):\n",
        "  \n",
        "  x_scaler = MinMaxScaler()\n",
        "\n",
        "  train_features = pd.DataFrame(x_scaler.fit_transform(df[features_to_scale]))\n",
        "  train_features.columns = features_to_scale\n",
        "#   train_labels = pd.DataFrame(x_scaler.fit_transform(df[[label]]))\n",
        "  train_labels = pd.DataFrame(df[[label]]/label_scale_factor)\n",
        "  train_labels.columns = [label]\n",
        "\n",
        "  #convert back to series\n",
        "  train_labels = train_labels[label]\n",
        "  \n",
        "  return train_features, train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-uLmqBsDvQl_",
        "colab_type": "code",
        "outputId": "ddd42ef7-d708-45a2-a9be-32b2edac0cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "test_features, test_labels = scale_dataframe_features_labels(df=test_df, features_to_scale=bands, label=label)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "M4amqMCAPcbI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_tfrecord_normalize(example_proto):\n",
        "  parsed_features = tf.parse_single_example(example_proto, featuresDict)\n",
        "  \n",
        "  for key in bands:\n",
        "    Xmax = raw_describe[key]['max']\n",
        "    Xmin = raw_describe[key]['min']\n",
        "    X_scaled = (parsed_features[key] - Xmin) / (Xmax - Xmin)\n",
        "    parsed_features[key] = X_scaled\n",
        "    \n",
        "    labels = parsed_features.pop(label)\n",
        "    # Scale the target to have a max value of ~ 100000\n",
        "    labels = labels / label_scale_factor\n",
        "    return parsed_features, tf.cast(labels, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-462EO_T-FJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# transformed_dataset, transform_fn = (\n",
        "#     (parsedDataset, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n",
        "#         preprocessing_fn))\n",
        "# transformed_data, transformed_metadata = transformed_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5kgYwmEBGQ9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Make an input function\n",
        "\n",
        "def tfrecord_input_fn(fileName,\n",
        "                      numEpochs=None,\n",
        "                      shuffle=True,\n",
        "                      batchSize=None):\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset(fileName, compression_type='GZIP')\n",
        "\n",
        "  # Map the parsing function over the dataset\n",
        "  dataset = dataset.map(parse_tfrecord_normalize, num_parallel_calls=5)\n",
        "  \n",
        "  # normalize (alternative way)\n",
        "#   dataset = dataset.map(_batch_normalization)\n",
        "\n",
        "\n",
        "  # Shuffle, batch, and repeat.\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=batchSize * 2)\n",
        "    \n",
        "  dataset = dataset.batch(batchSize).repeat(numEpochs)\n",
        "\n",
        "  # Make a one-shot iterator.\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  features, labels = iterator.get_next()\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5244jp8xRDR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Input function specific to Pandas DataFrame\n",
        "\n",
        "def pd_input_fn(X, y=None, num_epochs=None, shuffle=True, batch_size=1000):  \n",
        "    return tf.estimator.inputs.pandas_input_fn(x=X,\n",
        "                                               y=y,\n",
        "                                               num_epochs=num_epochs,\n",
        "                                               shuffle=shuffle,\n",
        "                                               batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTtYZTaZG-4d",
        "colab_type": "code",
        "outputId": "795d295b-1720-4ed3-8392-ab7de6fbba6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2145
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Make and train a classifier (Best version)\n",
        "\n",
        "inputColumns = {tf.feature_column.numeric_column(k) for k in bands}\n",
        "TYPES = [tf.float32 for k in bands] \n",
        "\n",
        "learning_rate = 0.005\n",
        "clip_norm = 5.0\n",
        "\n",
        "# optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
        "\n",
        "optimizer = tf.train.ProximalAdagradOptimizer(\n",
        "    learning_rate=learning_rate,\n",
        "    l1_regularization_strength=0.001)\n",
        "\n",
        "optimizer = tf.contrib.estimator.clip_gradients_by_norm(\n",
        "    optimizer, clip_norm)\n",
        "\n",
        "hu = [8,10]\n",
        "bs = 15000\n",
        "steps = 5000\n",
        "periods = 100\n",
        "steps_per_period = steps / periods\n",
        "ep = 100\n",
        "extra = 'xx'\n",
        "\n",
        "hu_str = ''.join(str(e) for e in hu)\n",
        "\n",
        "model_run = '/lr' + str(learning_rate)[2:] + '_bs' + str(bs) + '_e' + str(ep) + '_lyr' + hu_str + extra\n",
        "\n",
        "regressor = tf.estimator.DNNRegressor(feature_columns=inputColumns,\n",
        "                                  hidden_units=hu,\n",
        "                                  model_dir=model_dir + model_run,\n",
        "                                  optimizer=optimizer,\n",
        "                                  batch_norm=False\n",
        "                                      )\n",
        "\n",
        "feature_spec = tf.feature_column.make_parse_example_spec(inputColumns)\n",
        "export_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n",
        "servable_model_dir = \"/tmp/saved_model\"\n",
        "\n",
        "# def json_serving_input_fn(): \n",
        "#   \"\"\"Build the serving inputs.\"\"\" \n",
        "#   inputs = {} \n",
        "#   for feat, dtype in zip(bands, TYPES): \n",
        "#     inputs[feat] = tf.placeholder(shape=[None], dtype=dtype) \n",
        "\n",
        "#   features = {\n",
        "#     key: tf.expand_dims(tensor, -1)\n",
        "#     for key, tensor in inputs.items()\n",
        "#   }\n",
        "#   return tf.contrib.learn.InputFnOps(features, None, inputs)\n",
        "\n",
        "\n",
        "evaluations = []  \n",
        "\n",
        "for i in range(5):  \n",
        "    regressor.train(input_fn=lambda: tfrecord_input_fn(fileName=train_file, numEpochs=ep, batchSize=bs))\n",
        "#     evaluations.append(regressor.evaluate(input_fn=pd_input_fn(test_features, test_labels, num_epochs=1, shuffle=True, batch_size=bs)))\n",
        "      \n",
        "servable_model_path = regressor.export_savedmodel(servable_model_dir, export_input_fn)\n",
        "\n",
        "# regressor.export_savedmodel(\"saved_model\",json_serving_input_fn)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5ce078d6d0>, '_model_dir': '/tmp/BUI2/lr005_bs15000_e100_lyr810xx', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/feature_column/feature_column.py:800: _parse_example_spec (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:loss = 27614083000.0, step = 0\n",
            "INFO:tensorflow:global_step/sec: 2.35568\n",
            "INFO:tensorflow:loss = 207290380.0, step = 100 (42.460 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.34534\n",
            "INFO:tensorflow:loss = 123419360.0, step = 200 (42.635 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.36922\n",
            "INFO:tensorflow:loss = 24010816.0, step = 300 (42.205 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 400 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1918.3232.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt-400\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 400 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:loss = 154798510.0, step = 400\n",
            "INFO:tensorflow:global_step/sec: 2.33972\n",
            "INFO:tensorflow:loss = 131233500.0, step = 500 (42.747 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.34519\n",
            "INFO:tensorflow:loss = 11298466.0, step = 600 (42.638 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.3528\n",
            "INFO:tensorflow:loss = 238252100.0, step = 700 (42.506 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 800 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 22612.037.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt-800\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 800 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:loss = 88562580.0, step = 800\n",
            "INFO:tensorflow:global_step/sec: 2.35737\n",
            "INFO:tensorflow:loss = 28693556.0, step = 900 (42.427 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.37228\n",
            "INFO:tensorflow:loss = 85521270.0, step = 1000 (42.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.376\n",
            "INFO:tensorflow:loss = 136023780.0, step = 1100 (42.084 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1200 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 2036.8558.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt-1200\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1200 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:loss = 264732270.0, step = 1200\n",
            "INFO:tensorflow:global_step/sec: 2.38512\n",
            "INFO:tensorflow:loss = 192269470.0, step = 1300 (41.928 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.37619\n",
            "INFO:tensorflow:loss = 187058580.0, step = 1400 (42.085 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.37068\n",
            "INFO:tensorflow:loss = 23786184.0, step = 1500 (42.186 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1600 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 47811.664.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt-1600\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1600 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "INFO:tensorflow:loss = 52948990.0, step = 1600\n",
            "INFO:tensorflow:global_step/sec: 2.40249\n",
            "INFO:tensorflow:loss = 96718890.0, step = 1700 (41.631 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.41502\n",
            "INFO:tensorflow:loss = 185531600.0, step = 1800 (41.403 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.3924\n",
            "INFO:tensorflow:loss = 62464430.0, step = 1900 (41.799 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Loss for final step: 13409.876.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['serving_default', 'regression']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Restoring parameters from /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt-2000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: /tmp/saved_model/temp-1551893603/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n_QnR9tx36EJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def infer_input_fn(fileNames, side=256, batchSize=100):\n",
        "  \n",
        "  ds = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')\n",
        "  \n",
        "#   feature_columns = {}\n",
        "\n",
        "#   for k in bands:\n",
        "#     feature_columns[k] = tf.FixedLenFeature([side,side], dtype=tf.float32)\n",
        "\n",
        "  feature_columns = {\n",
        "    'nlcd_urban': tf.FixedLenFeature([side,side], dtype=tf.float32),  \n",
        "    'pop_den': tf.FixedLenFeature([side,side], dtype=tf.float32),  \n",
        "    'nighttime_lights': tf.FixedLenFeature([side,side], dtype=tf.float32),    \n",
        "    'accessibility': tf.FixedLenFeature([side,side], dtype=tf.float32),\n",
        "  }\n",
        "  \n",
        "  def parse_normalize(example_proto):\n",
        "    parsed_features = tf.parse_single_example(example_proto, feature_columns)\n",
        "\n",
        "    for key in bands:\n",
        "      Xmax = raw_describe[key]['max']\n",
        "      Xmin = raw_describe[key]['min']\n",
        "      X_scaled = (parsed_features[key] - Xmin) / (Xmax - Xmin)\n",
        "      parsed_features[key] = X_scaled\n",
        "\n",
        "      return parsed_features\n",
        "  \n",
        "  ds = ds.map(parse_normalize, num_parallel_calls=5).batch(batchSize)\n",
        "  \n",
        "  iterator = ds.make_one_shot_iterator()\n",
        "  features = iterator.get_next()\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJ12ztLX4Czx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_example(pred_dict):\n",
        "  predictions = pred_dict['predictions']\n",
        "  return tf.train.Example(\n",
        "    features=tf.train.Features(\n",
        "      feature={\n",
        "        'prediction': tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(\n",
        "                value=[1]))\n",
        "      }\n",
        "    )\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEC4sOzdBh4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is now done above in an automated fashion\n",
        "# predict_files = ['gs://geo_ai/BUI_image_9a26cef21ab34f6257d0a250882124fc00000.tfrecord.gz', 'gs://geo_ai/BUI_image_9a26cef21ab34f6257d0a250882124fc00001.tfrecord.gz']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNzaq3YZ4INn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predictions = regressor.predict(input_fn=lambda: infer_input_fn(\n",
        "#     fileNames=predict_files, \n",
        "#     batchSize=1,\n",
        "#     side=256),\n",
        "#     yield_single_examples=False)\n",
        "\n",
        "\n",
        "# MAX_RECORDS_PER_FILE = 50\n",
        "# output_path = 'gs://geo_ai/BUI_prediction_CONUS-{:05}.tfrecord'\n",
        "\n",
        "# # Create the records we'll ingest into EE\n",
        "# file_number = 0\n",
        "# still_writing = True\n",
        "# total_patches = 0\n",
        "# while still_writing:\n",
        "#   file_path = output_path.format(file_number)\n",
        "#   writer = tf.python_io.TFRecordWriter(file_path)\n",
        "#   print(\"Writing file: {f}\".format(f=file_path))\n",
        "#   try:\n",
        "#     written_records = 0\n",
        "#     while True:\n",
        "#       print(\"In the True loop\")\n",
        "#       pred_dict = predictions.next()\n",
        "#       print('pred_dict')\n",
        "      \n",
        "#       writer.write(make_example(pred_dict).SerializeToString())\n",
        "      \n",
        "#       written_records += 1 \n",
        "#       total_patches += 1\n",
        "      \n",
        "#       if written_records % 5 == 0:\n",
        "#         print(\"  Writing patch: {x}\".format(x=written_records))\n",
        "      \n",
        "#       if written_records == MAX_RECORDS_PER_FILE:\n",
        "#         break\n",
        "#   except: \n",
        "#     # Stop writing for any exception. Note that reaching the end of the prediction\n",
        "#     # dataset throws an exception.\n",
        "#     still_writing=False\n",
        "#   finally:\n",
        "#     file_number += 1\n",
        "#     writer.close()\n",
        "  \n",
        "# print('Wrote: {f} patches.'.format(f=total_patches))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihua5ubjAWi1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Alternate patch prediction method\n",
        "# You have to know the following from your export.\n",
        "PATCH_WIDTH = 256\n",
        "PATCH_HEIGHT = 256\n",
        "PATCH_DIMENSIONS_FLAT = [PATCH_WIDTH * PATCH_HEIGHT, 1]\n",
        "\n",
        "# Note that the tensors are in the shape of a patch, one patch for each band.\n",
        "columns = [\n",
        "  tf.FixedLenFeature(shape=PATCH_DIMENSIONS_FLAT, dtype=tf.float32) for k in bands\n",
        "]\n",
        "\n",
        "featuresDict = dict(zip(bands, columns))\n",
        "\n",
        "    \n",
        "# This input function reads in the TFRecord files exported from an image.\n",
        "# Note that because the pixels are arranged in patches, we need some additional\n",
        "# code to reshape the tensors.\n",
        "def predict_input_fn(fileNames):\n",
        "  \n",
        "  # Note that you can make one dataset from many files by specifying a list.\n",
        "  dataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')\n",
        "  \n",
        "  def parse_normalize(example_proto):\n",
        "    parsed_features = tf.parse_single_example(example_proto, featuresDict)\n",
        "\n",
        "    for key in bands:\n",
        "      Xmax = raw_describe[key]['max']\n",
        "      Xmin = raw_describe[key]['min']\n",
        "      X_scaled = (parsed_features[key] - Xmin) / (Xmax - Xmin)\n",
        "      parsed_features[key] = X_scaled\n",
        "\n",
        "      return parsed_features\n",
        "    \n",
        "#   def parse_image(example_proto):\n",
        "#     parsed_features = tf.parse_single_example(example_proto, featuresDict)\n",
        "#     return parsed_features\n",
        "  \n",
        "  dataset = dataset.map(parse_normalize, num_parallel_calls=5)\n",
        "\n",
        "  # Break our long tensors into many littler ones\n",
        "  dataset = dataset.flat_map(lambda features: tf.data.Dataset.from_tensor_slices(features))\n",
        "  \n",
        "  \n",
        "  # Read in batches corresponding to patch size.\n",
        "  dataset = dataset.batch(PATCH_WIDTH * PATCH_HEIGHT)\n",
        "  \n",
        "  # Make a one-shot iterator.\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  return iterator.get_next()\n",
        "\n",
        "# Do the prediction from the trained classifier.\n",
        "predictions = regressor.predict(\n",
        "  input_fn=lambda: predict_input_fn(predict_files)\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqcFzXdRNzOb",
        "colab_type": "code",
        "outputId": "a3e1acc7-318b-4b80-b4b8-431063968f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Define output names\n",
        "\n",
        "username = 'dylanhatlas'\n",
        "baseName = 'gs://geo_ai/BUI_'\n",
        "outputImageFile = baseName + 'TFpredictions_CONUS.TFRecord'\n",
        "outputJsonFile = baseName + 'image_' + link + 'mixer.json'\n",
        "print('Writing to: ' + outputImageFile)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing to: gs://geo_ai/BUI_TFpredictions_CONUS.TFRecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fB7I3NMCNSkl",
        "colab_type": "code",
        "outputId": "68483f5f-6d40-4ae7-e1eb-a555ff152e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7446
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Make predictions on the image data, write to a file\n",
        "\n",
        "import itertools\n",
        "\n",
        "iter1, iter2 = itertools.tee(predictions, 2)\n",
        "\n",
        "# Iterate over the predictions, printing the class_ids and posteriors.\n",
        "# This is just to examine the first prediction.\n",
        "for pred_dict in iter1:\n",
        "  print(pred_dict)\n",
        "  break # OK\n",
        "\n",
        "# Instantiate the writer.\n",
        "writer = tf.python_io.TFRecordWriter(outputImageFile)\n",
        "  \n",
        "# Every patch-worth of predictions we'll dump an example into the output\n",
        "# file with a single feature that holds our predictions. Since are predictions\n",
        "# are already in the order of the exported data, our patches we create here\n",
        "# will also be in the right order.\n",
        "patch = [[]] #[[], [], [], []]\n",
        "curPatch = 1\n",
        "for pred_dict in iter2:\n",
        "  patch[0].append(pred_dict['predictions'])\n",
        "#   patch[0].append(pred_dict['class_ids'])\n",
        "#   patch[1].append(pred_dict['probabilities'][0])\n",
        "#   patch[2].append(pred_dict['probabilities'][1])\n",
        "#   patch[3].append(pred_dict['probabilities'][2])\n",
        "  # Once we've seen a patches-worth of class_ids...\n",
        "  if (len(patch[0]) == PATCH_WIDTH * PATCH_HEIGHT):\n",
        "    \n",
        "    print('Done with patch ' + str(curPatch) + '...')\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Create an example\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'prediction': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=patch[0]))\n",
        "            \n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example to the file and clear our patch array so it's ready for\n",
        "    # another batch of class ids\n",
        "    writer.write(example.SerializeToString())\n",
        "    patch = [[]]\n",
        "    curPatch += 1 \n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/BUI2/lr005_bs15000_e100_lyr810xx/model.ckpt-2000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "{'predictions': array([0.01496847], dtype=float32)}\n",
            "Done with patch 1...\n",
            "Done with patch 2...\n",
            "Done with patch 3...\n",
            "Done with patch 4...\n",
            "Done with patch 5...\n",
            "Done with patch 6...\n",
            "Done with patch 7...\n",
            "Done with patch 8...\n",
            "Done with patch 9...\n",
            "Done with patch 10...\n",
            "Done with patch 11...\n",
            "Done with patch 12...\n",
            "Done with patch 13...\n",
            "Done with patch 14...\n",
            "Done with patch 15...\n",
            "Done with patch 16...\n",
            "Done with patch 17...\n",
            "Done with patch 18...\n",
            "Done with patch 19...\n",
            "Done with patch 20...\n",
            "Done with patch 21...\n",
            "Done with patch 22...\n",
            "Done with patch 23...\n",
            "Done with patch 24...\n",
            "Done with patch 25...\n",
            "Done with patch 26...\n",
            "Done with patch 27...\n",
            "Done with patch 28...\n",
            "Done with patch 29...\n",
            "Done with patch 30...\n",
            "Done with patch 31...\n",
            "Done with patch 32...\n",
            "Done with patch 33...\n",
            "Done with patch 34...\n",
            "Done with patch 35...\n",
            "Done with patch 36...\n",
            "Done with patch 37...\n",
            "Done with patch 38...\n",
            "Done with patch 39...\n",
            "Done with patch 40...\n",
            "Done with patch 41...\n",
            "Done with patch 42...\n",
            "Done with patch 43...\n",
            "Done with patch 44...\n",
            "Done with patch 45...\n",
            "Done with patch 46...\n",
            "Done with patch 47...\n",
            "Done with patch 48...\n",
            "Done with patch 49...\n",
            "Done with patch 50...\n",
            "Done with patch 51...\n",
            "Done with patch 52...\n",
            "Done with patch 53...\n",
            "Done with patch 54...\n",
            "Done with patch 55...\n",
            "Done with patch 56...\n",
            "Done with patch 57...\n",
            "Done with patch 58...\n",
            "Done with patch 59...\n",
            "Done with patch 60...\n",
            "Done with patch 61...\n",
            "Done with patch 62...\n",
            "Done with patch 63...\n",
            "Done with patch 64...\n",
            "Done with patch 65...\n",
            "Done with patch 66...\n",
            "Done with patch 67...\n",
            "Done with patch 68...\n",
            "Done with patch 69...\n",
            "Done with patch 70...\n",
            "Done with patch 71...\n",
            "Done with patch 72...\n",
            "Done with patch 73...\n",
            "Done with patch 74...\n",
            "Done with patch 75...\n",
            "Done with patch 76...\n",
            "Done with patch 77...\n",
            "Done with patch 78...\n",
            "Done with patch 79...\n",
            "Done with patch 80...\n",
            "Done with patch 81...\n",
            "Done with patch 82...\n",
            "Done with patch 83...\n",
            "Done with patch 84...\n",
            "Done with patch 85...\n",
            "Done with patch 86...\n",
            "Done with patch 87...\n",
            "Done with patch 88...\n",
            "Done with patch 89...\n",
            "Done with patch 90...\n",
            "Done with patch 91...\n",
            "Done with patch 92...\n",
            "Done with patch 93...\n",
            "Done with patch 94...\n",
            "Done with patch 95...\n",
            "Done with patch 96...\n",
            "Done with patch 97...\n",
            "Done with patch 98...\n",
            "Done with patch 99...\n",
            "Done with patch 100...\n",
            "Done with patch 101...\n",
            "Done with patch 102...\n",
            "Done with patch 103...\n",
            "Done with patch 104...\n",
            "Done with patch 105...\n",
            "Done with patch 106...\n",
            "Done with patch 107...\n",
            "Done with patch 108...\n",
            "Done with patch 109...\n",
            "Done with patch 110...\n",
            "Done with patch 111...\n",
            "Done with patch 112...\n",
            "Done with patch 113...\n",
            "Done with patch 114...\n",
            "Done with patch 115...\n",
            "Done with patch 116...\n",
            "Done with patch 117...\n",
            "Done with patch 118...\n",
            "Done with patch 119...\n",
            "Done with patch 120...\n",
            "Done with patch 121...\n",
            "Done with patch 122...\n",
            "Done with patch 123...\n",
            "Done with patch 124...\n",
            "Done with patch 125...\n",
            "Done with patch 126...\n",
            "Done with patch 127...\n",
            "Done with patch 128...\n",
            "Done with patch 129...\n",
            "Done with patch 130...\n",
            "Done with patch 131...\n",
            "Done with patch 132...\n",
            "Done with patch 133...\n",
            "Done with patch 134...\n",
            "Done with patch 135...\n",
            "Done with patch 136...\n",
            "Done with patch 137...\n",
            "Done with patch 138...\n",
            "Done with patch 139...\n",
            "Done with patch 140...\n",
            "Done with patch 141...\n",
            "Done with patch 142...\n",
            "Done with patch 143...\n",
            "Done with patch 144...\n",
            "Done with patch 145...\n",
            "Done with patch 146...\n",
            "Done with patch 147...\n",
            "Done with patch 148...\n",
            "Done with patch 149...\n",
            "Done with patch 150...\n",
            "Done with patch 151...\n",
            "Done with patch 152...\n",
            "Done with patch 153...\n",
            "Done with patch 154...\n",
            "Done with patch 155...\n",
            "Done with patch 156...\n",
            "Done with patch 157...\n",
            "Done with patch 158...\n",
            "Done with patch 159...\n",
            "Done with patch 160...\n",
            "Done with patch 161...\n",
            "Done with patch 162...\n",
            "Done with patch 163...\n",
            "Done with patch 164...\n",
            "Done with patch 165...\n",
            "Done with patch 166...\n",
            "Done with patch 167...\n",
            "Done with patch 168...\n",
            "Done with patch 169...\n",
            "Done with patch 170...\n",
            "Done with patch 171...\n",
            "Done with patch 172...\n",
            "Done with patch 173...\n",
            "Done with patch 174...\n",
            "Done with patch 175...\n",
            "Done with patch 176...\n",
            "Done with patch 177...\n",
            "Done with patch 178...\n",
            "Done with patch 179...\n",
            "Done with patch 180...\n",
            "Done with patch 181...\n",
            "Done with patch 182...\n",
            "Done with patch 183...\n",
            "Done with patch 184...\n",
            "Done with patch 185...\n",
            "Done with patch 186...\n",
            "Done with patch 187...\n",
            "Done with patch 188...\n",
            "Done with patch 189...\n",
            "Done with patch 190...\n",
            "Done with patch 191...\n",
            "Done with patch 192...\n",
            "Done with patch 193...\n",
            "Done with patch 194...\n",
            "Done with patch 195...\n",
            "Done with patch 196...\n",
            "Done with patch 197...\n",
            "Done with patch 198...\n",
            "Done with patch 199...\n",
            "Done with patch 200...\n",
            "Done with patch 201...\n",
            "Done with patch 202...\n",
            "Done with patch 203...\n",
            "Done with patch 204...\n",
            "Done with patch 205...\n",
            "Done with patch 206...\n",
            "Done with patch 207...\n",
            "Done with patch 208...\n",
            "Done with patch 209...\n",
            "Done with patch 210...\n",
            "Done with patch 211...\n",
            "Done with patch 212...\n",
            "Done with patch 213...\n",
            "Done with patch 214...\n",
            "Done with patch 215...\n",
            "Done with patch 216...\n",
            "Done with patch 217...\n",
            "Done with patch 218...\n",
            "Done with patch 219...\n",
            "Done with patch 220...\n",
            "Done with patch 221...\n",
            "Done with patch 222...\n",
            "Done with patch 223...\n",
            "Done with patch 224...\n",
            "Done with patch 225...\n",
            "Done with patch 226...\n",
            "Done with patch 227...\n",
            "Done with patch 228...\n",
            "Done with patch 229...\n",
            "Done with patch 230...\n",
            "Done with patch 231...\n",
            "Done with patch 232...\n",
            "Done with patch 233...\n",
            "Done with patch 234...\n",
            "Done with patch 235...\n",
            "Done with patch 236...\n",
            "Done with patch 237...\n",
            "Done with patch 238...\n",
            "Done with patch 239...\n",
            "Done with patch 240...\n",
            "Done with patch 241...\n",
            "Done with patch 242...\n",
            "Done with patch 243...\n",
            "Done with patch 244...\n",
            "Done with patch 245...\n",
            "Done with patch 246...\n",
            "Done with patch 247...\n",
            "Done with patch 248...\n",
            "Done with patch 249...\n",
            "Done with patch 250...\n",
            "Done with patch 251...\n",
            "Done with patch 252...\n",
            "Done with patch 253...\n",
            "Done with patch 254...\n",
            "Done with patch 255...\n",
            "Done with patch 256...\n",
            "Done with patch 257...\n",
            "Done with patch 258...\n",
            "Done with patch 259...\n",
            "Done with patch 260...\n",
            "Done with patch 261...\n",
            "Done with patch 262...\n",
            "Done with patch 263...\n",
            "Done with patch 264...\n",
            "Done with patch 265...\n",
            "Done with patch 266...\n",
            "Done with patch 267...\n",
            "Done with patch 268...\n",
            "Done with patch 269...\n",
            "Done with patch 270...\n",
            "Done with patch 271...\n",
            "Done with patch 272...\n",
            "Done with patch 273...\n",
            "Done with patch 274...\n",
            "Done with patch 275...\n",
            "Done with patch 276...\n",
            "Done with patch 277...\n",
            "Done with patch 278...\n",
            "Done with patch 279...\n",
            "Done with patch 280...\n",
            "Done with patch 281...\n",
            "Done with patch 282...\n",
            "Done with patch 283...\n",
            "Done with patch 284...\n",
            "Done with patch 285...\n",
            "Done with patch 286...\n",
            "Done with patch 287...\n",
            "Done with patch 288...\n",
            "Done with patch 289...\n",
            "Done with patch 290...\n",
            "Done with patch 291...\n",
            "Done with patch 292...\n",
            "Done with patch 293...\n",
            "Done with patch 294...\n",
            "Done with patch 295...\n",
            "Done with patch 296...\n",
            "Done with patch 297...\n",
            "Done with patch 298...\n",
            "Done with patch 299...\n",
            "Done with patch 300...\n",
            "Done with patch 301...\n",
            "Done with patch 302...\n",
            "Done with patch 303...\n",
            "Done with patch 304...\n",
            "Done with patch 305...\n",
            "Done with patch 306...\n",
            "Done with patch 307...\n",
            "Done with patch 308...\n",
            "Done with patch 309...\n",
            "Done with patch 310...\n",
            "Done with patch 311...\n",
            "Done with patch 312...\n",
            "Done with patch 313...\n",
            "Done with patch 314...\n",
            "Done with patch 315...\n",
            "Done with patch 316...\n",
            "Done with patch 317...\n",
            "Done with patch 318...\n",
            "Done with patch 319...\n",
            "Done with patch 320...\n",
            "Done with patch 321...\n",
            "Done with patch 322...\n",
            "Done with patch 323...\n",
            "Done with patch 324...\n",
            "Done with patch 325...\n",
            "Done with patch 326...\n",
            "Done with patch 327...\n",
            "Done with patch 328...\n",
            "Done with patch 329...\n",
            "Done with patch 330...\n",
            "Done with patch 331...\n",
            "Done with patch 332...\n",
            "Done with patch 333...\n",
            "Done with patch 334...\n",
            "Done with patch 335...\n",
            "Done with patch 336...\n",
            "Done with patch 337...\n",
            "Done with patch 338...\n",
            "Done with patch 339...\n",
            "Done with patch 340...\n",
            "Done with patch 341...\n",
            "Done with patch 342...\n",
            "Done with patch 343...\n",
            "Done with patch 344...\n",
            "Done with patch 345...\n",
            "Done with patch 346...\n",
            "Done with patch 347...\n",
            "Done with patch 348...\n",
            "Done with patch 349...\n",
            "Done with patch 350...\n",
            "Done with patch 351...\n",
            "Done with patch 352...\n",
            "Done with patch 353...\n",
            "Done with patch 354...\n",
            "Done with patch 355...\n",
            "Done with patch 356...\n",
            "Done with patch 357...\n",
            "Done with patch 358...\n",
            "Done with patch 359...\n",
            "Done with patch 360...\n",
            "Done with patch 361...\n",
            "Done with patch 362...\n",
            "Done with patch 363...\n",
            "Done with patch 364...\n",
            "Done with patch 365...\n",
            "Done with patch 366...\n",
            "Done with patch 367...\n",
            "Done with patch 368...\n",
            "Done with patch 369...\n",
            "Done with patch 370...\n",
            "Done with patch 371...\n",
            "Done with patch 372...\n",
            "Done with patch 373...\n",
            "Done with patch 374...\n",
            "Done with patch 375...\n",
            "Done with patch 376...\n",
            "Done with patch 377...\n",
            "Done with patch 378...\n",
            "Done with patch 379...\n",
            "Done with patch 380...\n",
            "Done with patch 381...\n",
            "Done with patch 382...\n",
            "Done with patch 383...\n",
            "Done with patch 384...\n",
            "Done with patch 385...\n",
            "Done with patch 386...\n",
            "Done with patch 387...\n",
            "Done with patch 388...\n",
            "Done with patch 389...\n",
            "Done with patch 390...\n",
            "Done with patch 391...\n",
            "Done with patch 392...\n",
            "Done with patch 393...\n",
            "Done with patch 394...\n",
            "Done with patch 395...\n",
            "Done with patch 396...\n",
            "Done with patch 397...\n",
            "Done with patch 398...\n",
            "Done with patch 399...\n",
            "Done with patch 400...\n",
            "Done with patch 401...\n",
            "Done with patch 402...\n",
            "Done with patch 403...\n",
            "Done with patch 404...\n",
            "Done with patch 405...\n",
            "Done with patch 406...\n",
            "Done with patch 407...\n",
            "Done with patch 408...\n",
            "Done with patch 409...\n",
            "Done with patch 410...\n",
            "Done with patch 411...\n",
            "Done with patch 412...\n",
            "Done with patch 413...\n",
            "Done with patch 414...\n",
            "Done with patch 415...\n",
            "Done with patch 416...\n",
            "Done with patch 417...\n",
            "Done with patch 418...\n",
            "Done with patch 419...\n",
            "Done with patch 420...\n",
            "Done with patch 421...\n",
            "Done with patch 422...\n",
            "Done with patch 423...\n",
            "Done with patch 424...\n",
            "Done with patch 425...\n",
            "Done with patch 426...\n",
            "Done with patch 427...\n",
            "Done with patch 428...\n",
            "Done with patch 429...\n",
            "Done with patch 430...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QgmBdtsYVmZ-",
        "colab_type": "code",
        "outputId": "2e99838e-8ce4-4163-9a87-a34d06c529cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Upload the predicted image to Earth Engine\n",
        "\n",
        "outputAssetID = 'users/dylanhatlas/DeepLearning/TF_BUI_predictions_CONUS' \n",
        "# outputImageFile = baseName + 'TFpredictions_CONUS_v1.TFRecord'\n",
        "!earthengine upload image --asset_id={outputAssetID} {outputImageFile} {outputJsonFile}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started upload task with ID: XH3DKDIXIQOOUOZUVY33H3UI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5poYwwQKXLot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# export_dir = '/tmp/saved_model/1551814652/'\n",
        "\n",
        "#   new_saver = tf.train.import_meta_graph('/tmp/saved_model/1551814652/saved_model.meta')\n",
        "#   new_saver.restore(sess, 'saved_model')\n",
        "  \n",
        "# with tf.Session(graph=tf.Graph()) as sess:\n",
        "#   tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], export_dir)\n",
        "#   graph = tf.get_default_graph()\n",
        "#   x = graph.get_tensor_by_name(\"x:0\")\n",
        "#   model = graph.get_tensor_by_name(\"finalnode:0\")\n",
        "#   print(sess.run(model, {x: test_file}))\n",
        "    \n",
        "# from tensorflow.contrib import predictor\n",
        "\n",
        "# predict_fn = predictor.from_saved_model(export_dir)\n",
        "\n",
        "# predictions = predict_fn(input_fn=lambda: tfrecord_input_fn(fileName=test_file, numEpochs=1, batchSize=10000, shuffle=False))\n",
        "# print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UAsXGRxufBP7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# see this https://stackoverflow.com/questions/48836502/training-loss-not-steadily-decreasing-using-tensorflow-dataset-api\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuvZqW0696cz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}